{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(os.path.dirname(os.path.dirname(os.getcwd())),'sd-4sql\\\\packages'))\n",
    "datasets_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd()))),'Data\\\\original-data\\\\')\n",
    "saved_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd()))),'Data\\\\saved-data\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preparation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'LOAD AND HANDLE THE SQL QUERIES DATASET'\n",
    "url = datasets_path + 'sql-queries.csv'\n",
    "df_req = pd.read_csv(url)\n",
    "df_req['time'] = pd.to_datetime(df_req['time']).dt.tz_localize(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'LOAD AND HANDLE ORACLE ASH DATASET'\n",
    "url = datasets_path + 'oracle-ash.csv'\n",
    "df_oracle_ash = pd.read_csv(url)\n",
    "df_oracle_ash['dateFin'] = pd.to_datetime(df_oracle_ash['dateFin']).dt.tz_localize(None)\n",
    "df_oracle_ash['dateDebut'] = df_oracle_ash['dateFin'].astype('datetime64[ns]') - timedelta(hours=0, minutes=5)\n",
    "df_oracle_ash.fillna(0, inplace=True)\n",
    "df_oracle_ash.iloc[:,2:-1] = df_oracle_ash.iloc[:,2:-1].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'CONSTRUCT THE QUERIES TABLE'\n",
    "df_req_ash = pd.merge(df_req, df_oracle_ash, on='instanceCode')\n",
    "df_req_ash = df_req_ash[(df_req_ash.time < df_req_ash.dateFin) & (df_req_ash.time > df_req_ash.dateDebut)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_req_ash = df_req_ash.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'LOAD THE INSTANCES PROPERTIES DATASET'\n",
    "url = datasets_path + 'env-features.csv'\n",
    "df_inst = pd.read_csv(url)\n",
    "\n",
    "'JOIN THE TWO TABLES'\n",
    "df_req_inst = df_req_ash.join(df_inst.set_index('Instance code'), on='instanceCode')\n",
    "\n",
    "'HANDLE THE RESULTING TABLE'\n",
    "df_req_inst.drop(columns = ['Instance lib','Version majeure de Copilote lib'], inplace = True)\n",
    "df_req_inst.rename(columns = {\n",
    "                            \"Déclinaison de Copilote\": \"DeclinaisonCOP\",\n",
    "                            \"Version majeure de Copilote code\" : \"versionMajCode\",\n",
    "                            \"Version de Copilote\" : \"versionCOP\",\n",
    "                            \"Type BDD\" : \"typeBDD\",\n",
    "                            \"Version BDD\" : \"versionBDD\",\n",
    "                            \"Pool JDBC max\" : \"poolJDBC_MAX\",\n",
    "                            \"Pool JDBC min\" : \"poolJDBC_MIN\",\n",
    "                            \"Mémoire vive de la BDD\" : \"memoireVive\",\n",
    "                            \"Max SGA\" : \"sga_MAX\",\n",
    "                            \"Nb de CPU de la BDD\" : \"nbCPU\",\n",
    "                            \"Nombre max de curseurs pour une session\" : \"nbCurseurs_MAX\",\n",
    "                            \"Nombre max de processus BDD\" : \"nbProcessus_MAX\"\n",
    "                           },inplace=True)\n",
    "\n",
    "df_req_inst['memoireVive'] = pd.to_numeric(df_req_inst['memoireVive'].str.split(',', expand = True)[0], errors='coerce')\n",
    "df_req_inst['sga_MAX'] = pd.to_numeric(df_req_inst['sga_MAX'].str.split(',', expand = True)[0], errors='coerce')\n",
    "df_req_inst['versionMajCode'] = df_req_inst['versionMajCode'].str.replace('V7','V07')\n",
    "df_req_inst['versionMajCode'] = df_req_inst['versionMajCode'].str.replace('V8','V08')\n",
    "df_req_inst['versionMajCode'] = df_req_inst['versionMajCode'].str.replace('V9','V09')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'LOAD AND HANDLE THE ALERTS DATASET'\n",
    "url = datasets_path + 'alerts.csv'\n",
    "df_alertes = pd.read_csv(url)\n",
    "df_alertes['dateDebut'] = pd.to_datetime(df_alertes['dateDebut'])\n",
    "df_alertes['dateFin'] = pd.to_datetime(df_alertes['dateFin'])\n",
    "\n",
    "'MERGE THE ALERTS TABLE'\n",
    "types_alertes = list(df_alertes['metrique'].unique())\n",
    "dict_alertes = dict.fromkeys(types_alertes,[])\n",
    "\n",
    "for i in range (df_req_inst.shape[0]) :\n",
    "    #print(i)\n",
    "    time = df_req_inst['time'].iloc[i]\n",
    "    instance = df_req_inst['instanceCode'].iloc[i]\n",
    "    df_tmp = df_alertes[(df_alertes['instanceCode'] == instance) & (time > df_alertes['dateDebut']) & (time < df_alertes['dateFin'])]\n",
    "\n",
    "    for key in dict_alertes :\n",
    "        if key in df_tmp['metrique'].values :\n",
    "            dict_alertes[key] = dict_alertes.get(key,[]) + [df_tmp[df_tmp['metrique'] == key]['niveauAlerte'].values[0]]\n",
    "        else : \n",
    "            dict_alertes[key] = dict_alertes.get(key,[]) + ['None']\n",
    "\n",
    "for key in dict_alertes :\n",
    "    df_req_inst[key] = pd.Series(dict_alertes[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'CONSTRUCT THE QUERIES TABLE'\n",
    "df_req_inst['hour'] = df_req_inst['time'].dt.hour\n",
    "df_req_inst['day'] = df_req_inst['time'].dt.day_name()\n",
    "df_req_inst.drop(columns = ['time','dateFin','dateDebut'], inplace = True)\n",
    "df_req_inst.reset_index(inplace = True,drop = True)\n",
    "df_req_inst['requete'] = df_req_inst['requete'].str.lower()\n",
    "df_req_inst['requete'] = df_req_inst['requete'].str.replace(':','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'HIBERNATE QUERIES TABLE'\n",
    "requetes_hib = df_req_inst[df_req_inst['requete'].str.contains('fr.infologic')]\n",
    "requetes_hib['typeRequete'] = requetes_hib['requete'].apply(lambda x : x.split(' ')[0])\n",
    "requetes_hib.to_csv(saved_path + 'requetes_hib.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "'HIBERNATE SELECT QUERIES TABLE'\n",
    "\n",
    "requetes_select = requetes_hib[requetes_hib['typeRequete'] == 'select'].reset_index(drop = True)\n",
    "del requetes_select['typeRequete']\n",
    "del requetes_select['typeBDD']\n",
    "\n",
    "requetes_select['requete'] = requetes_select['requete'].str.replace('?','0')\n",
    "requetes_select['requete'] = requetes_select['requete'].str.replace(' new','')\n",
    "requetes_select['requete'] = requetes_select['requete'].str.replace('join fetch','join')\n",
    "requetes_select['requete'] = requetes_select['requete'].str.replace('where exists \\(','where exists ( select * ')\n",
    "requetes_select['requete'] = requetes_select['requete'].str.replace('\\[ 0 ','')\n",
    "requetes_select['requete'] = requetes_select['requete'].str.replace(']','')\n",
    "requetes_select['requete'] = requetes_select['requete'].str.replace('mon,','mon join ')\n",
    "requetes_select['requete'] = requetes_select['requete'].str.replace('\\(from','(select * from')\n",
    "\n",
    "sub_req = 'a.lot.ik = p3 and a.unitlogis.ik = p4 or a.lot.ik = p5 and a.unitlogis.ik = p6 or a.lot.ik = p7 and a.unitlogis.ik = p8 or a.lot.ik = p9 and a.unitlogis.ik = p10 or a.lot.ik = p11 and a.unitlogis.ik = p12 or a.lot.ik = p13 and a.unitlogis.ik = p14 or a.lot.ik = p15 and a.unitlogis.ik = p16 or a.lot.ik = p17 and a.unitlogis.ik = p18 or a.lot.ik = p19 and a.unitlogis.ik = p20 or a.lot.ik = p21'\n",
    "cond_A = requetes_select['requete'].str.contains('order by')\n",
    "cond_B = requetes_select['requete'].str.contains('group by')\n",
    "cond_C = requetes_select['requete'].str.find ('order by') < requetes_select['requete'].str.find ('group by')\n",
    "cond_D = requetes_select['requete'].str.contains(', fr.infologic')\n",
    "cond_E = requetes_select['requete'].str.contains('join') \n",
    "cond_F = (requetes_select['requete'].str.find(', fr.infologic') > requetes_select['requete'].str.find('join'))\n",
    "cond_G = requetes_select['requete'].str.contains('elements')\n",
    "\n",
    "requetes_select.loc[cond_A & cond_B & cond_C,'requete'] = requetes_select.loc[cond_A & cond_B & cond_C,'requete'].apply(lambda x : swap_orderby_groupby(x))\n",
    "requetes_select.loc[cond_D & cond_E & cond_F,'requete'] = requetes_select.loc[cond_D & cond_E & cond_F,'requete'].str.replace(', fr.infologic',' join fr.infologic')\n",
    "requetes_select.loc[cond_G,'requete'] = requetes_select.loc[cond_G,'requete'].apply(lambda x : transform_elements(x))\n",
    "\n",
    "requetes_select = requetes_select.drop(requetes_select[requetes_select['requete'].str.contains(sub_req)].index)\n",
    "requetes_select = requetes_select.drop(requetes_select[requetes_select['requete'].str.contains('xmlelement')].index)\n",
    "requetes_select = requetes_select.drop(requetes_select[requetes_select['requete'].str.contains(' b.prod=')].index)\n",
    "requetes_select = requetes_select.drop(requetes_select[requetes_select['requete'].str.len() > 120000].index)\n",
    "requetes_select = requetes_select.drop(requetes_select[requetes_select['requete'].str.contains('as float')].index).reset_index(drop = True)\n",
    "requetes_select['long'] = requetes_select['requete'].str.len()\n",
    "requetes_select[df_oracle_ash.columns[2:-1]] = (requetes_select[df_oracle_ash.columns[2:-1]]/60).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = ['day','hour','requete','nbLignes','long','durationMS','instanceCode'] + list(requetes_select.columns[16:32]) + list(requetes_select.columns[4:16])\n",
    "requetes_select = requetes_select[new_columns]\n",
    "columns = ['day','hour','query','nrows','length','time','serverName','declination',\n",
    "           'codeVersion','softwareVersion','dbVersion','jdbcMax','jdbcMin','dbMemory','sgaMax',\n",
    "           'nbCPU','dbCursorsMax','dbProcessMax','nbUsers','anomalyASH','manyActiveSessions','blockedSessions',\n",
    "           'poolAlmostFull'] + list(requetes_select.columns[-12:])\n",
    "requetes_select.columns = columns\n",
    "requetes_select.to_csv(saved_path + 'queries_hib_select.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'insert into gl_tabletemp(ik) values(?)'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_req[df_req['requete'].str.contains('insert')]['requete'][201454]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
