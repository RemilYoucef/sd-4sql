{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from moz_sql_parser import parse\n",
    "sys.setrecursionlimit(3000)\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(os.path.dirname(os.path.dirname(os.getcwd())),'sd-4sql\\\\packages'))\n",
    "saved_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd()))),'Data\\\\saved-data\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from query_parser import *\n",
    "from sd_analysis import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'LOAD THE DATASET'\n",
    "url = saved_path + 'queries_hib_select.csv'\n",
    "queries = pd.read_csv(url, index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cond1 = queries['query'].str.contains('join')\n",
    "cond2 = queries['query'].str.contains('count')\n",
    "cond3 = queries['query'].str.contains('in \\( select')\n",
    "queries[cond1 &cond2& cond3]['query'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_uniques = queries[queries.time >= 0]['query'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_uniques = queries[queries.time >= 0]['query'].value_counts()\n",
    "pd.DataFrame({'query':queries_uniques.index, 'frequency':queries_uniques.values}).to_csv(saved_path + 'queries_uniques.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_query_parse   =  dict.fromkeys(queries_uniques.index, None)\n",
    "dict_query_tables  =  dict.fromkeys(queries_uniques.index, None)\n",
    "dict_query_from    =  dict.fromkeys(queries_uniques.index, None)\n",
    "dict_query_join    =  dict.fromkeys(queries_uniques.index, None)\n",
    "dict_query_alias   =  dict.fromkeys(queries_uniques.index, None)\n",
    "dict_query_projs   =  dict.fromkeys(queries_uniques.index, None)\n",
    "dict_query_atts    =  dict.fromkeys(queries_uniques.index, None)\n",
    "dict_query_orderby =  dict.fromkeys(queries_uniques.index, None)\n",
    "dict_query_groupby =  dict.fromkeys(queries_uniques.index, None)\n",
    "dict_query_having  =  dict.fromkeys(queries_uniques.index, None)\n",
    "\n",
    "for i in range(queries_uniques.size) :\n",
    "    query_parsed = parse(queries_uniques.index[i])\n",
    "    dict_query_parse   [queries_uniques.index[i]] = query_parsed\n",
    "    dict_query_tables  [queries_uniques.index[i]] = get_tables(query_parsed,{})[0]\n",
    "    dict_query_alias   [queries_uniques.index[i]] = get_tables(query_parsed,{})[1]\n",
    "    dict_query_from    [queries_uniques.index[i]] = get_tables_fj(query_parsed,{})[0]\n",
    "    dict_query_join    [queries_uniques.index[i]] = get_tables_fj(query_parsed,{})[1]\n",
    "    dict_query_projs   [queries_uniques.index[i]] = get_projections(query_parsed, get_tables(query_parsed,{})[1])\n",
    "    dict_query_atts    [queries_uniques.index[i]] = get_atts_where(query_parsed, get_tables(query_parsed,{})[1])\n",
    "    dict_query_orderby [queries_uniques.index[i]] = get_atts_order_by(query_parsed, get_tables(query_parsed,{})[1])\n",
    "    dict_query_groupby [queries_uniques.index[i]] = get_atts_group_by(query_parsed, get_tables(query_parsed,{})[1])\n",
    "    dict_query_having  [queries_uniques.index[i]] = get_atts_having(query_parsed, get_tables(query_parsed,{})[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'SAVE THE DICTIONNARIES'\n",
    "\n",
    "with open(saved_path + 'query_parsed.pickle', 'wb') as handle:\n",
    "    pickle.dump(dict_query_parse, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(saved_path + 'query_tables.pickle', 'wb') as handle:\n",
    "    pickle.dump(dict_query_tables, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(saved_path + 'query_alias.pickle', 'wb') as handle:\n",
    "    pickle.dump(dict_query_alias, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(saved_path + 'query_from.pickle', 'wb') as handle:\n",
    "    pickle.dump(dict_query_from, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(saved_path + 'query_join.pickle', 'wb') as handle:\n",
    "    pickle.dump(dict_query_join, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(saved_path + 'query_projs.pickle', 'wb') as handle:\n",
    "    pickle.dump(dict_query_projs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(saved_path + 'query_atts.pickle', 'wb') as handle:\n",
    "    pickle.dump(dict_query_atts, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(saved_path + 'query_orderby.pickle', 'wb') as handle:\n",
    "    pickle.dump(dict_query_orderby, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(saved_path + 'query_groupby.pickle', 'wb') as handle:\n",
    "    pickle.dump(dict_query_groupby, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(saved_path + 'query_having.pickle', 'wb') as handle:\n",
    "    pickle.dump(dict_query_having, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tables = pd.DataFrame(columns=['table','time'])\n",
    "\n",
    "for query, tables in dict_query_tables.items():\n",
    "    durations = list(queries[queries['query'] == query]['time'].values)\n",
    "    for elt in itertools.product(tables,durations) :\n",
    "        new_row = {'table' : elt[0], 'time' : elt[1]}\n",
    "        df_tables = df_tables.append(new_row, ignore_index = True)\n",
    "#df_tables.to_csv(saved_path + 'df_tables.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tables_stat = pd.DataFrame(columns=['table','min','max','moy','med','mad','var'])\n",
    "df_tables_stat['table'] = df_tables.groupby('table', as_index=False)['time'].mean()['table']\n",
    "df_tables_stat['min'] = df_tables.groupby('table', as_index=False)['time'].min()['time']\n",
    "df_tables_stat['max'] = df_tables.groupby('table', as_index=False)['time'].max()['time']\n",
    "df_tables_stat['med'] = df_tables.groupby('table', as_index=False)['time'].median()['time']\n",
    "df_tables_stat['moy'] = df_tables.groupby('table', as_index=False)['time'].mean()['time']\n",
    "df_tables_stat['var'] = df_tables.groupby('table', as_index=False)['time'].var()['time']\n",
    "df_tables_stat['mad'] = df_tables.groupby('table', as_index=False)['time'].mad()['time']\n",
    "#df_tables_stat.to_csv(saved_path + 'df_tables_stat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from = pd.DataFrame(columns=['table','time'])\n",
    "\n",
    "for query, tables in dict_query_from.items():\n",
    "    durations = list(queries[queries['query'] == query]['time'].values)\n",
    "    for elt in itertools.product(tables,durations) :\n",
    "        new_row = {'table' : elt[0], 'time' : elt[1]}\n",
    "        df_from = df_from.append(new_row, ignore_index = True)\n",
    "df_from.to_csv(saved_path + 'df_from.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join = pd.DataFrame(columns=['table','time'])\n",
    "\n",
    "for query, tables in dict_query_join.items():\n",
    "    durations = list(queries[queries['query'] == query]['time'].values)\n",
    "    for elt in itertools.product(tables,durations) :\n",
    "        new_row = {'table' : elt[0], 'time' : elt[1]}\n",
    "        df_join = df_join.append(new_row, ignore_index = True)\n",
    "df_join.to_csv(saved_path + 'df_join.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_projs = pd.DataFrame(columns=['proj','time'])\n",
    "\n",
    "for query, projs in dict_query_projs.items():\n",
    "\n",
    "    durations = list(queries[queries['query'] == query]['time'].values)\n",
    "    for elt in itertools.product(projs,durations) :\n",
    "        new_row = {'proj' : elt[0], 'time' : elt[1]}\n",
    "        df_projs = df_projs.append(new_row, ignore_index = True)\n",
    "df_projs.to_csv(saved_path + 'df_projs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atts = pd.DataFrame(columns=['att','time'])\n",
    "\n",
    "for query, atts in dict_query_atts.items():\n",
    "\n",
    "    durations = list(queries[queries['query'] == query]['time'].values)\n",
    "    for elt in itertools.product(atts,durations) :\n",
    "        new_row = {'att' : elt[0], 'time' : elt[1]}\n",
    "        df_atts = df_atts.append(new_row, ignore_index = True)\n",
    "df_atts.to_csv(saved_path + 'df_atts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atts_orderby = pd.DataFrame(columns=['att_orderby','time'])\n",
    "\n",
    "for query, atts_orderby in dict_query_orderby.items():\n",
    "\n",
    "    durations = list(queries[queries['query'] == query]['time'].values)\n",
    "    if atts_orderby : \n",
    "        for elt in itertools.product(atts_orderby,durations) :\n",
    "            new_row = {'att_orderby' : elt[0], 'time' : elt[1]}\n",
    "            df_atts_orderby = df_atts_orderby.append(new_row, ignore_index = True)\n",
    "df_atts_orderby.to_csv(saved_path + 'df_atts_orderby.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atts_groupby = pd.DataFrame(columns=['att_groupby','time'])\n",
    "\n",
    "for query, atts_groupby in dict_query_groupby.items():\n",
    "\n",
    "    durations = list(queries[queries['query'] == query]['time'].values)\n",
    "    if atts_groupby :\n",
    "        for elt in itertools.product(atts_groupby,durations) :\n",
    "            new_row = {'att_groupby' : elt[0], 'time' : elt[1]}\n",
    "            df_atts_groupby = df_atts_groupby.append(new_row, ignore_index = True)\n",
    "df_atts_groupby.to_csv(saved_path + 'df_atts_groupby.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atts_having = pd.DataFrame(columns=['att_having','time'])\n",
    "\n",
    "for query, atts_having in dict_query_having.items():\n",
    "\n",
    "    durations = list(queries[queries['query'] == query]['time'].values)\n",
    "    if atts_having : \n",
    "        for elt in itertools.product(atts_having, durations) :\n",
    "            new_row = {'att_having' : elt[0], 'time' : elt[1]}\n",
    "            df_atts_having = df_atts_having.append(new_row, ignore_index = True)\n",
    "df_atts_having.to_csv(saved_path + 'df_atts_having.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_all_tables = []\n",
    "for table in list(dict_query_tables.values()) :\n",
    "    list_all_tables = list_all_tables + table\n",
    "\n",
    "list_all_from = []\n",
    "for table in list(dict_query_from.values()) :\n",
    "    list_all_from = list_all_from + table\n",
    "    \n",
    "list_all_join = []\n",
    "for table in list(dict_query_join.values()) :\n",
    "    list_all_join = list_all_join + table\n",
    "\n",
    "list_all_projections = []\n",
    "for proj in list(dict_query_projs.values()) :\n",
    "    list_all_projections = list_all_projections + proj\n",
    "\n",
    "list_all_atts = []\n",
    "for att in list(dict_query_atts.values()) :\n",
    "    list_all_atts = list_all_atts + att  \n",
    "    \n",
    "list_all_atts_having = []\n",
    "for att_having in list(dict_query_having.values()) :\n",
    "    list_all_atts_having  = list_all_atts_having  + att_having    \n",
    "\n",
    "list_all_atts_groupby = []\n",
    "for att_groupby in list(dict_query_groupby.values()) :\n",
    "    list_all_atts_groupby  = list_all_atts_groupby  + att_groupby    \n",
    "    \n",
    "list_all_atts_orderby = []\n",
    "for att_orderby in list(dict_query_orderby.values()) :\n",
    "    list_all_atts_orderby  = list_all_atts_orderby  + att_orderby  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_tables  = ['TABLE_'+ s for s in list(set(list_all_tables))]\n",
    "distinct_from    = ['FROM_'+ s for s in list(set(list_all_from))]\n",
    "distinct_join    = ['JOIN_'+ s for s in list(set(list_all_join))]\n",
    "distinct_projs   = ['SELECT_' + s for s in list(set(list_all_projections))]\n",
    "distinct_where   = ['WHERE_' + s for s in list(set(list_all_atts))]\n",
    "distinct_having  = ['HAVING_' + s for s in list(set(list_all_atts_having))]\n",
    "distinct_orderby = ['ORDERBY_' + s for s in list(set(list_all_atts_orderby))]\n",
    "distinct_groupby = ['GROUPBY_' + s for s in list(set(list_all_atts_groupby))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8676"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features = distinct_from + distinct_join + distinct_projs + distinct_where + distinct_having + distinct_orderby + distinct_groupby\n",
    "len(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_projs = pd.DataFrame(0, index=np.arange(queries.shape[0]), columns = list(set(list_all_projections)))\n",
    "for i in range(queries.shape[0]):\n",
    "    projs_req = Counter(dict_query_projs[queries.loc[i,'query']])\n",
    "    for proj in projs_req : \n",
    "        queries_projs.loc[i,proj] = projs_req[proj]\n",
    "queries_projs.columns = distinct_projs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_tables = pd.DataFrame(0, index=np.arange(queries.shape[0]), columns = list(set(list_all_tables)))\n",
    "for i in range(queries.shape[0]):\n",
    "    tables_req = Counter(dict_query_tables[queries.loc[i,'query']])\n",
    "    for table in tables_req : \n",
    "        queries_tables.loc[i,table] = tables_req[table]\n",
    "queries_tables.columns = distinct_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_from = pd.DataFrame(0, index=np.arange(queries.shape[0]), columns = list(set(list_all_from)))\n",
    "for i in range(queries.shape[0]):\n",
    "    from_req = Counter(dict_query_from[queries.loc[i,'query']])\n",
    "    for table in from_req : \n",
    "        queries_from.loc[i,table] = from_req[table]\n",
    "queries_from.columns = distinct_from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_join = pd.DataFrame(0, index=np.arange(queries.shape[0]), columns = list(set(list_all_join)))\n",
    "for i in range(queries.shape[0]):\n",
    "    join_req = Counter(dict_query_join[queries.loc[i,'query']])\n",
    "    for table in join_req : \n",
    "        queries_join.loc[i,table] = join_req[table]\n",
    "queries_join.columns = distinct_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_atts = pd.DataFrame(0, index=np.arange(queries.shape[0]), columns = list(set(list_all_atts)))\n",
    "for i in range(queries.shape[0]):\n",
    "    atts_req = Counter(dict_query_atts[queries.loc[i,'query']])\n",
    "    for att in atts_req : \n",
    "        queries_atts.loc[i,att] = atts_req[att]\n",
    "queries_atts.columns = distinct_where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_atts_having = pd.DataFrame(0, index=np.arange(queries.shape[0]), columns = list(set(list_all_atts_having)))\n",
    "for i in range(queries.shape[0]):\n",
    "    atts_having_req = Counter(dict_query_having[queries.loc[i,'query']])\n",
    "    for att_having in atts_having_req : \n",
    "        queries_atts_having.loc[i,att_having] = atts_having_req[att_having]\n",
    "queries_atts_having.columns = distinct_having"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_atts_orderby = pd.DataFrame(0, index=np.arange(queries.shape[0]), columns = list(set(list_all_atts_orderby)))\n",
    "for i in range(queries.shape[0]):\n",
    "    atts_orderby_req = Counter(dict_query_orderby[queries.loc[i,'query']])\n",
    "    for att_orderby in atts_orderby_req : \n",
    "        queries_atts_orderby.loc[i,att_orderby] = atts_orderby_req[att_orderby]\n",
    "queries_atts_orderby.columns = distinct_orderby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_atts_groupby = pd.DataFrame(0, index=np.arange(queries.shape[0]), columns = list(set(list_all_atts_groupby)))\n",
    "for i in range(queries.shape[0]):\n",
    "    atts_groupby_req = Counter(dict_query_groupby[queries.loc[i,'query']])\n",
    "    for att_groupby in atts_groupby_req : \n",
    "        queries_atts_groupby.loc[i,att_groupby] = atts_groupby_req[att_groupby]\n",
    "queries_atts_groupby.columns = distinct_groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_all = pd.concat([queries, queries_projs, queries_from, queries_join, queries_atts, queries_atts_having, queries_atts_orderby, queries_atts_groupby], axis=1)\n",
    "queries_all.to_csv(saved_path + 'queries_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'GROUP BY QUERY AND INSTANCE FOR INSPECTION'\n",
    "\n",
    "df_queries_instances = pd.DataFrame()\n",
    "\n",
    "df_queries_instances['query'] = queries.groupby(['query','serverName'], as_index=False).size()['query']\n",
    "\n",
    "df_queries_instances['from_tables'] = df_queries_instances['query'].apply(lambda x : dict_query_from[x])\n",
    "df_queries_instances['join_tables'] = df_queries_instances['query'].apply(lambda x : dict_query_join[x])\n",
    "df_queries_instances['projections'] = df_queries_instances['query'].apply(lambda x : dict_query_projs[x])\n",
    "df_queries_instances['atts_where']  = df_queries_instances['query'].apply(lambda x : dict_query_atts[x])\n",
    "df_queries_instances['atts_orderby'] = df_queries_instances['query'].apply(lambda x : dict_query_orderby[x])\n",
    "df_queries_instances['atts_groupby'] = df_queries_instances['query'].apply(lambda x : dict_query_groupby[x])\n",
    "df_queries_instances['atts_having'] = df_queries_instances['query'].apply(lambda x : dict_query_having[x])\n",
    "\n",
    "df_queries_instances['serverName'] = queries.groupby(['query','serverName'], as_index=False).size()['serverName']\n",
    "df_queries_instances['nbUtilsateurs'] = queries.groupby(['query','serverName','nbUsers'], as_index=False).size()['nbUsers']\n",
    "df_queries_instances['nbqueries_inst'] = queries.groupby(['query','serverName'], as_index=False).size()['size']\n",
    "\n",
    "df_queries_instances['moynrows'] = queries.groupby(['query','serverName'], as_index=False)['nrows'].mean()['nrows']\n",
    "df_queries_instances['mednrows'] = queries.groupby(['query','serverName'], as_index=False)['nrows'].median()['nrows']\n",
    "df_queries_instances['minnrows'] = queries.groupby(['query','serverName'], as_index=False)['nrows'].min()['nrows']\n",
    "df_queries_instances['maxnrows'] = queries.groupby(['query','serverName'], as_index=False)['nrows'].max()['nrows']\n",
    "\n",
    "df_queries_instances['moytime'] = queries.groupby(['query','serverName'], as_index=False)['time'].mean()['time']\n",
    "df_queries_instances['medtime'] = queries.groupby(['query','serverName'], as_index=False)['time'].median()['time']\n",
    "df_queries_instances['mintime'] = queries.groupby(['query','serverName'], as_index=False)['time'].min()['time']\n",
    "df_queries_instances['maxtime'] = queries.groupby(['query','serverName'], as_index=False)['time'].max()['time']\n",
    "\n",
    "df_queries_instances = df_queries_instances.round(0)\n",
    "cols = df_queries_instances.columns[11:]\n",
    "df_queries_instances[cols] = df_queries_instances[cols].applymap(np.int64)\n",
    "\n",
    "df_queries_instances.to_csv(saved_path + 'queries_per_servers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPERIMENTS DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "'D1 declination == \"Ventes\" AND nbUsers > 100'\n",
    "get_df_conditions(queries_all, {'nbUsers':{'gt':100},'declination':'Ventes'}).to_csv(saved_path + 'dataset-d1.csv',\n",
    "                                                                                               index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "'D2 table == \"MVTREALISE\"'\n",
    "get_df_table(queries_all, 'fr.infologic.stocks.gestion.modele.mvtrealise').to_csv(saved_path +'dataset-d2.csv', \n",
    "                                                                                  index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "'D3 softwareVersion == \"v15\"'\n",
    "get_df_conditions(queries_all, {'softwareVersion' : 'V15_2L.d211415.21/10/2020 16:48'}).to_csv(saved_path +'dataset-d3.csv',\n",
    "                                                                                               index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "'D4 servers with significant blocking sessions alerts'\n",
    "def discretize_alert (x) :\n",
    "    if x == 'Critique' or x == 'Bloquant' or x == 'Alarme':\n",
    "        return 1 \n",
    "    else :\n",
    "        return 0\n",
    "\n",
    "queries_all['blockedSessions_disc'] = queries['blockedSessions'].apply(lambda x : discretize_alert(x))\n",
    "queries_alertes = queries_all[queries_all['serverName'].str.contains(('|'.join(queries_all[queries_all['blockedSessions_disc'] > 0]['serverName'].value_counts().index)))]\n",
    "queries_alertes = queries_alertes.loc[:, (queries_alertes != 0).any(axis=0)]\n",
    "for column in queries_alertes.columns :\n",
    "    if queries_alertes[column].unique().size == 1 : \n",
    "        del queries_alertes[column]\n",
    "queries_alertes.to_csv(saved_path +'dataset-d4.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = X_train_df.append([X_train_df[X_train_df['chain'] == 'INFO']]*3,ignore_index=True)\n",
    "X_train_df = X_train_df.append([X_train_df[X_train_df['chain'] == 'MOBI']]*3,ignore_index=True)\n",
    "X_train_df = X_train_df.append([X_train_df[X_train_df['chain'] == 'ETAT']]*3,ignore_index=True)\n",
    "X_train_df = X_train_df.append([X_train_df[X_train_df['chain'] == 'WEB' ]]*6,ignore_index=True)\n",
    "X_train_df = X_train_df.append([X_train_df[X_train_df['chain'] == 'CRM']] *7,ignore_index=True)\n",
    "X_train_df = X_train_df.append([X_train_df[X_train_df['chain'] == 'LO']]  *7,ignore_index=True)\n",
    "X_train_df = X_train_df.append([X_train_df[X_train_df['chain'] == 'ACTI']]*7,ignore_index=True)\n",
    "X_train_df = X_train_df.append([X_train_df[X_train_df['chain'] == 'DEMAT']]*9,ignore_index=True)\n",
    "X_train_df = X_train_df.append([X_train_df[X_train_df['chain'] == 'VTPREV']]*9,ignore_index=True)\n",
    "X_train_df = X_train_df.append([X_train_df[X_train_df['chain'] == 'PO']]*10,ignore_index=True)\n",
    "X_train_df = X_train_df.append([X_train_df[X_train_df['chain'] == 'ALCOOL']]*10,ignore_index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
